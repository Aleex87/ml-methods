{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41b96d18",
   "metadata": {},
   "source": [
    "# Deep Learning – Regression (PyTorch)\n",
    "\n",
    "This notebook is part of the **ML-Methods** project.\n",
    "\n",
    "It introduces **Deep Learning for supervised regression**\n",
    "using the PyTorch framework.\n",
    "\n",
    "As with all other notebooks in this project,\n",
    "the initial sections focus on data preparation\n",
    "and are intentionally repeated.\n",
    "\n",
    "This ensures:\n",
    "- consistency across models\n",
    "- fair comparison of results\n",
    "- a unified learning pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ce0e6e",
   "metadata": {},
   "source": [
    "_____\n",
    "\n",
    "## Notebook Roadmap (standard ML-Methods)\n",
    "\n",
    "1. Project setup and common pipeline  \n",
    "2. Dataset loading  \n",
    "3. Train-test split  \n",
    "4. Feature scaling (why we do it)  \n",
    "\n",
    "----------------------------------\n",
    "\n",
    "5. What is this model? (Intuition)  \n",
    "6. Model training  \n",
    "7. Model behavior and key parameters  \n",
    "8. Predictions  \n",
    "9. Model evaluation  \n",
    "10. When to use it and when not to  \n",
    "11. Model persistence  \n",
    "12. Mathematical formulation (deep dive)  \n",
    "13. Final summary – Code only\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7c4651",
   "metadata": {},
   "source": [
    "___\n",
    "## How this notebook should be read\n",
    "\n",
    "This notebook is designed to be read **top to bottom**.\n",
    "\n",
    "Before every code cell, you will find a short explanation describing:\n",
    "- what we are about to do\n",
    "- why this step is necessary\n",
    "- how it fits into the overall process\n",
    "\n",
    "Compared to scikit-learn,\n",
    "this notebook exposes more of the **training mechanics**.\n",
    "\n",
    "The goal is to understand:\n",
    "- how deep learning regression works internally\n",
    "- how training is controlled explicitly\n",
    "- how PyTorch differs from high-level abstractions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580cc30",
   "metadata": {},
   "source": [
    "___\n",
    "## What is Deep Learning (in this context)?\n",
    "\n",
    "In this notebook,\n",
    "Deep Learning refers to **neural networks trained manually**\n",
    "using PyTorch.\n",
    "\n",
    "Unlike scikit-learn:\n",
    "- the training loop is explicit\n",
    "- forward and backward passes are visible\n",
    "- optimization is controlled step by step\n",
    "\n",
    "This provides deeper insight\n",
    "into how regression models actually learn.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85631b3",
   "metadata": {},
   "source": [
    "___\n",
    "## What do we want to achieve?\n",
    "\n",
    "Our objective is to train a neural network that:\n",
    "- takes numerical input features\n",
    "- processes them through multiple layers\n",
    "- outputs a **single continuous value**\n",
    "\n",
    "The model learns a mapping:\n",
    "\n",
    "input features → numerical target\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cda0ee",
   "metadata": {},
   "source": [
    "___\n",
    "## Why use PyTorch for regression?\n",
    "\n",
    "PyTorch is a **low-level deep learning framework**\n",
    "that provides fine-grained control over training.\n",
    "\n",
    "Using PyTorch allows us to:\n",
    "- see the forward pass explicitly\n",
    "- control loss computation\n",
    "- manage gradients manually\n",
    "- understand optimization mechanics\n",
    "\n",
    "This notebook represents the **next conceptual step**\n",
    "after scikit-learn:\n",
    "from abstraction → understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7466793a",
   "metadata": {},
   "source": [
    "___\n",
    "## What you should expect from the results\n",
    "\n",
    "With Deep Learning regression in PyTorch,\n",
    "you should expect:\n",
    "\n",
    "- non-linear regression capability\n",
    "- flexible model architecture\n",
    "- full control over training dynamics\n",
    "- behavior similar to scikit-learn MLP\n",
    "\n",
    "However:\n",
    "- code is more verbose\n",
    "- more responsibility is on the user\n",
    "- mistakes are easier to make\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945bbe67",
   "metadata": {},
   "source": [
    "___\n",
    "## 1. Project setup and common pipeline\n",
    "\n",
    "In this section we set up the common pipeline\n",
    "used across regression models in this project.\n",
    "\n",
    "Although the model is implemented in PyTorch,\n",
    "data preparation remains consistent\n",
    "with all other regression notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c55114ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Common imports used across regression models\n",
    "# ====================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    r2_score\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784d6599",
   "metadata": {},
   "source": [
    "### PyTorch vs scikit-learn (at a glance)\n",
    "\n",
    "Compared to scikit-learn:\n",
    "- models are defined as Python classes\n",
    "- training loops are written manually\n",
    "- gradients are handled explicitly\n",
    "\n",
    "The surrounding pipeline,\n",
    "however, remains unchanged.\n",
    "\n",
    "In the next section,\n",
    "we will load the regression dataset\n",
    "used throughout this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40964809",
   "metadata": {},
   "source": [
    "___\n",
    "## 2. Dataset loading\n",
    "\n",
    "In this section we load the dataset\n",
    "used for the deep learning regression task.\n",
    "\n",
    "We use the same regression dataset\n",
    "adopted in the other regression notebooks\n",
    "to ensure fair comparison across models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "549458f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Dataset loading\n",
    "# ====================================\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53347c1",
   "metadata": {},
   "source": [
    "### Inputs and target\n",
    "\n",
    "- `X` contains the input features\n",
    "- `y` contains the continuous target variable\n",
    "\n",
    "This is a **supervised regression problem**:\n",
    "- each input corresponds to a real-valued output\n",
    "- the goal is to predict a numerical quantity\n",
    "\n",
    "At this stage:\n",
    "- data is still in pandas format\n",
    "- no preprocessing has been applied yet\n",
    "\n",
    "In the next section,\n",
    "we will split the dataset\n",
    "into training and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4b13c4",
   "metadata": {},
   "source": [
    "____\n",
    "## 3. Train-test split\n",
    "\n",
    "In this section we split the dataset\n",
    "into training and test sets.\n",
    "\n",
    "This allows us to evaluate\n",
    "how well the neural network\n",
    "generalizes to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "722f013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Train-test split\n",
    "# ====================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251598b2",
   "metadata": {},
   "source": [
    "### Why this step is essential\n",
    "\n",
    "A regression model must be evaluated\n",
    "on data it has never seen during training.\n",
    "\n",
    "By separating the dataset:\n",
    "- the training set is used for learning\n",
    "- the test set is used only for evaluation\n",
    "\n",
    "This prevents data leakage\n",
    "and ensures realistic performance estimates.\n",
    "\n",
    "In the next section,\n",
    "we will apply feature scaling,\n",
    "which is mandatory for deep learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c05b8",
   "metadata": {},
   "source": [
    "___\n",
    "## 4. Feature scaling (why we do it)\n",
    "\n",
    "In this section we apply feature scaling\n",
    "to the input features.\n",
    "\n",
    "For deep learning regression models,\n",
    "feature scaling is **mandatory**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f123d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Feature scaling\n",
    "# ====================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19e544c",
   "metadata": {},
   "source": [
    "### Why we use standardization here\n",
    "\n",
    "Neural networks are trained\n",
    "using gradient-based optimization.\n",
    "\n",
    "Standardization:\n",
    "- centers features around zero\n",
    "- ensures comparable variance across features\n",
    "- improves numerical stability during training\n",
    "\n",
    "Without proper scaling:\n",
    "- gradients may explode or vanish\n",
    "- optimization may fail\n",
    "- training becomes unstable\n",
    "\n",
    "At this point:\n",
    "- data is in NumPy format\n",
    "- values are ready to be converted into tensors\n",
    "\n",
    "In the next section,\n",
    "we will explain **what the PyTorch model is**\n",
    "and how neural networks perform regression\n",
    "at a lower level.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5beea38",
   "metadata": {},
   "source": [
    "___\n",
    "## 5. What is this model? (Deep Learning Regression – PyTorch)\n",
    "\n",
    "Before writing any PyTorch code,\n",
    "it is important to understand\n",
    "what the model is conceptually doing.\n",
    "\n",
    "In regression,\n",
    "the goal is to predict a **continuous numerical value**\n",
    "from a set of input features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005fae8",
   "metadata": {},
   "source": [
    "### How regression works in a neural network\n",
    "\n",
    "A neural network for regression:\n",
    "- receives a vector of input features\n",
    "- transforms it through multiple layers\n",
    "- outputs a single real number\n",
    "\n",
    "The model learns a function:\n",
    "\n",
    "input vector → numerical output\n",
    "\n",
    "Unlike linear regression,\n",
    "this function is **non-linear**\n",
    "and learned progressively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7736a720",
   "metadata": {},
   "source": [
    "### What PyTorch adds conceptually\n",
    "\n",
    "With PyTorch:\n",
    "- we define the model explicitly\n",
    "- we control the forward pass\n",
    "- we decide how loss is computed\n",
    "- we update parameters manually\n",
    "\n",
    "This makes PyTorch ideal\n",
    "for understanding how learning happens,\n",
    "not just that it happens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801aff35",
   "metadata": {},
   "source": [
    "### High-level learning process\n",
    "\n",
    "Training follows a loop:\n",
    "1. forward pass (prediction)\n",
    "2. loss computation (error)\n",
    "3. backward pass (gradients)\n",
    "4. parameter update\n",
    "\n",
    "This loop is repeated\n",
    "until the model learns a good approximation\n",
    "of the regression function.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bba19f5",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "PyTorch regression models:\n",
    "- learn non-linear functions\n",
    "- expose training mechanics explicitly\n",
    "- behave similarly to scikit-learn models\n",
    "  when architecture and data are the same\n",
    "\n",
    "In the next section,\n",
    "we will define the neural network architecture\n",
    "using PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73277cd",
   "metadata": {},
   "source": [
    "___\n",
    "## 6. Model training (PyTorch Regression)\n",
    "\n",
    "In this section we define and train\n",
    "a neural network regressor using PyTorch.\n",
    "\n",
    "Unlike scikit-learn,\n",
    "both the model and the training loop\n",
    "must be written explicitly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3defebd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Model definition\n",
    "# ====================================\n",
    "\n",
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21cf0021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Training setup\n",
    "# ====================================\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "\n",
    "model = RegressionNet(input_dim)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33a3b41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Training loop\n",
    "# ====================================\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "epochs = 100\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837a0687",
   "metadata": {},
   "source": [
    "### What is happening during training\n",
    "\n",
    "- The model performs a forward pass\n",
    "- Predictions are compared to true values\n",
    "- Loss (MSE) measures prediction error\n",
    "- Gradients are computed automatically\n",
    "- Parameters are updated using Adam\n",
    "\n",
    "This explicit loop is the core of PyTorch.\n",
    "\n",
    "In the next section,\n",
    "we analyze model behavior\n",
    "and key parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ad8117",
   "metadata": {},
   "source": [
    "___\n",
    "## 7. Model behavior and key parameters\n",
    "\n",
    "In this section we analyze\n",
    "how the PyTorch regression model behaves\n",
    "and which parameters influence learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c08d9438",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "\n",
    "The network uses:\n",
    "- two hidden layers\n",
    "- ReLU activation\n",
    "- linear output layer\n",
    "\n",
    "This allows:\n",
    "- non-linear feature interactions\n",
    "- continuous output prediction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a779a121",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "We use Mean Squared Error (MSE):\n",
    "- penalizes large errors\n",
    "- standard choice for regression\n",
    "- differentiable and stable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d680304",
   "metadata": {},
   "source": [
    "### Optimizer behavior\n",
    "\n",
    "Adam optimizer:\n",
    "- adapts learning rates\n",
    "- speeds up convergence\n",
    "- works well for most regression tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597894e0",
   "metadata": {},
   "source": [
    "### Training duration\n",
    "\n",
    "More epochs:\n",
    "- improve learning initially\n",
    "- may cause overfitting if excessive\n",
    "\n",
    "Monitoring loss over epochs\n",
    "helps diagnose training behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170afb96",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "PyTorch gives full control\n",
    "over model behavior.\n",
    "\n",
    "This flexibility allows:\n",
    "- custom architectures\n",
    "- precise debugging\n",
    "- deeper understanding\n",
    "\n",
    "In the next section,\n",
    "we will generate predictions\n",
    "using the trained model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7782489",
   "metadata": {},
   "source": [
    "___\n",
    "## 8. Predictions\n",
    "\n",
    "In this section we use the trained PyTorch model\n",
    "to generate predictions on unseen test data.\n",
    "\n",
    "Predictions are continuous numerical values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b7d0076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Predictions\n",
    "# ====================================\n",
    "\n",
    "model.eval()\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred_tensor = model(X_test_tensor)\n",
    "\n",
    "y_pred = y_pred_tensor.numpy().flatten()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ba413a",
   "metadata": {},
   "source": [
    "### What happens during prediction\n",
    "\n",
    "- The model is set to evaluation mode\n",
    "- Gradients are disabled\n",
    "- The forward pass generates predictions\n",
    "\n",
    "This ensures:\n",
    "- faster inference\n",
    "- no gradient accumulation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8250841e",
   "metadata": {},
   "source": [
    "### What we have now\n",
    "\n",
    "At this point:\n",
    "- `y_test` contains true target values\n",
    "- `y_pred` contains predicted values\n",
    "\n",
    "These will be compared\n",
    "using regression metrics\n",
    "in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245c48df",
   "metadata": {},
   "source": [
    "___\n",
    "## 9. Model evaluation\n",
    "\n",
    "In this section we evaluate the performance\n",
    "of the PyTorch regression model\n",
    "on unseen test data.\n",
    "\n",
    "For regression problems,\n",
    "evaluation focuses on **prediction error**\n",
    "and **quality of fit**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3e550bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7457036463188563,\n",
       " np.float64(0.8635413402488942),\n",
       " 0.626965656223535,\n",
       " 0.4309382347792724)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ====================================\n",
    "# Regression evaluation metrics\n",
    "# ====================================\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, rmse, mae, r2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae3d95f",
   "metadata": {},
   "source": [
    "### How to read these results\n",
    "\n",
    "- **RMSE**  \n",
    "  Measures the typical prediction error\n",
    "  in the same unit as the target variable.\n",
    "\n",
    "- **MAE**  \n",
    "  Measures the average absolute error\n",
    "  and is more robust to outliers.\n",
    "\n",
    "- **R² score**  \n",
    "  Measures how much of the variance\n",
    "  in the target is explained by the model.\n",
    "\n",
    "These metrics together provide\n",
    "a complete view of regression performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c12e4",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "Evaluation metrics must always be computed\n",
    "on unseen data.\n",
    "\n",
    "A non-zero RMSE is expected\n",
    "and indicates that the model is generalizing,\n",
    "not memorizing the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a921f4f7",
   "metadata": {},
   "source": [
    "___\n",
    "## 10. When to use it and when not to\n",
    "\n",
    "Deep Learning regression with PyTorch\n",
    "is powerful but not always necessary.\n",
    "\n",
    "Choosing this approach depends on\n",
    "problem complexity and practical constraints.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939eff83",
   "metadata": {},
   "source": [
    "### When to use PyTorch for regression\n",
    "\n",
    "PyTorch regression is a good choice when:\n",
    "\n",
    "- relationships are highly non-linear\n",
    "- model architecture must be customized\n",
    "- training dynamics need full control\n",
    "- experimentation and research are required\n",
    "\n",
    "It is especially useful\n",
    "when building models from scratch\n",
    "or exploring novel architectures.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7c4ff",
   "metadata": {},
   "source": [
    "### When NOT to use PyTorch for regression\n",
    "\n",
    "PyTorch may not be ideal when:\n",
    "\n",
    "- the dataset is small\n",
    "- the problem is simple\n",
    "- rapid prototyping is needed\n",
    "- interpretability is critical\n",
    "\n",
    "In these cases,\n",
    "simpler models or scikit-learn\n",
    "are often more efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d61f1d2",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "PyTorch offers maximum flexibility\n",
    "at the cost of increased complexity.\n",
    "\n",
    "It should be chosen\n",
    "when control and understanding\n",
    "are more important than convenience.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369d917",
   "metadata": {},
   "source": [
    "___\n",
    "## 11. Model persistence\n",
    "\n",
    "In this section we save the trained PyTorch model\n",
    "and the preprocessing steps\n",
    "used during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949128c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Model persistence\n",
    "# ====================================\n",
    "\n",
    "model_dir = Path(\"models/supervised_learning/regression/deep_learning_pytorch\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save model state\n",
    "torch.save(model.state_dict(), model_dir / \"pytorch_regression_model.pt\")\n",
    "\n",
    "# Save scaler\n",
    "joblib.dump(scaler, model_dir / \"scaler.joblib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec5cd9",
   "metadata": {},
   "source": [
    "### What we have saved\n",
    "\n",
    "We saved:\n",
    "- the trained PyTorch model parameters\n",
    "- the feature scaler\n",
    "\n",
    "Together, these represent\n",
    "the complete regression pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0c6f3b",
   "metadata": {},
   "source": [
    "### Why saving the scaler matters\n",
    "\n",
    "Neural networks are sensitive\n",
    "to feature scaling.\n",
    "\n",
    "Using a different scaler\n",
    "would lead to inconsistent predictions.\n",
    "\n",
    "Saving the scaler ensures\n",
    "reproducibility and correctness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953136d5",
   "metadata": {},
   "source": [
    "___\n",
    "## 12. Mathematical formulation (deep dive)\n",
    "\n",
    "This section describes the mathematical principles\n",
    "behind deep learning regression\n",
    "implemented in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1283ba05",
   "metadata": {},
   "source": [
    "### Regression objective\n",
    "\n",
    "The dataset is represented as:\n",
    "\n",
    "$$\n",
    "\\{(x_i, y_i)\\}_{i=1}^n\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x_i \\in \\mathbb{R}^d$\n",
    "- $y_i \\in \\mathbb{R}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af71cd1",
   "metadata": {},
   "source": [
    "### Model as a function\n",
    "\n",
    "The neural network learns a function:\n",
    "\n",
    "$$\n",
    "\\hat{y} = f(x; \\theta)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\theta$ represents weights and biases\n",
    "- $\\hat{y}$ is the predicted value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b5a94",
   "metadata": {},
   "source": [
    "### Layer transformations\n",
    "\n",
    "Each hidden layer computes:\n",
    "\n",
    "$$\n",
    "h = \\text{ReLU}(Wx + b)\n",
    "$$\n",
    "\n",
    "The output layer is linear:\n",
    "\n",
    "$$\n",
    "\\hat{y} = W_{\\text{out}} h + b_{\\text{out}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bc8d3f",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Training minimizes Mean Squared Error:\n",
    "\n",
    "$$\n",
    "MSE = \\frac{1}{n} \\sum (y - \\hat{y})^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993981c7",
   "metadata": {},
   "source": [
    "### Optimization\n",
    "\n",
    "Gradients are computed via backpropagation,\n",
    "and parameters are updated using Adam:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta MSE\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fe16b9",
   "metadata": {},
   "source": [
    "### Final takeaway\n",
    "\n",
    "Deep learning regression can be viewed\n",
    "as non-linear function approximation\n",
    "optimized via gradient descent.\n",
    "\n",
    "PyTorch exposes these mechanisms explicitly,\n",
    "making learning transparent and flexible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c1bf11e",
   "metadata": {},
   "source": [
    "___\n",
    "## 13. Final summary – Code only\n",
    "\n",
    "The following cell contains the complete\n",
    "PyTorch regression pipeline.\n",
    "\n",
    "No explanations are provided here.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b1ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Imports\n",
    "# ====================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Dataset loading\n",
    "# ====================================\n",
    "\n",
    "data = fetch_california_housing(as_frame=True)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Train-test split\n",
    "# ====================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Feature scaling\n",
    "# ====================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Model definition\n",
    "# ====================================\n",
    "\n",
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.out = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        return self.out(x)\n",
    "\n",
    "\n",
    "input_dim = X_train_scaled.shape[1]\n",
    "model = RegressionNet(input_dim)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Training setup\n",
    "# ====================================\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Training loop\n",
    "# ====================================\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "for _ in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Predictions\n",
    "# ====================================\n",
    "\n",
    "model.eval()\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y_pred = model(X_test_tensor).numpy().flatten()\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Evaluation\n",
    "# ====================================\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "mse, rmse, mae, r2\n",
    "\n",
    "\n",
    "# ====================================\n",
    "# Model persistence\n",
    "# ====================================\n",
    "\n",
    "model_dir = Path(\"models/supervised_learning/regression/deep_learning_pytorch\")\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "torch.save(model.state_dict(), model_dir / \"pytorch_regression_model.pt\")\n",
    "joblib.dump(scaler, model_dir / \"scaler.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-env",
   "language": "python",
   "name": "pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
