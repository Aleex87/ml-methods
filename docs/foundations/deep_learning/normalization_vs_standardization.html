<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>normalization_vs_standardization</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="../../assets/css/style.css" />
</head>
<body>
<h1 id="normalization-vs-standardization-in-deep-learning">Normalization
vs Standardization in Deep Learning</h1>
<p>In deep learning, proper data scaling is critical.</p>
<p>Neural networks are trained using gradient descent. If input features
have very different scales, training can become unstable or
inefficient.</p>
<p>Normalization and standardization are two common preprocessing
techniques.</p>
<hr />
<h1 id="why-scaling-matters-in-neural-networks">Why Scaling Matters in
Neural Networks</h1>
<p>Neural networks rely on:</p>
<ul>
<li>weighted sums</li>
<li>activation functions</li>
<li>gradient-based optimization</li>
</ul>
<p>If input values are too large:</p>
<ul>
<li>gradients may explode</li>
<li>updates become unstable</li>
</ul>
<p>If input values are too small:</p>
<ul>
<li>gradients may vanish</li>
<li>learning becomes slow</li>
</ul>
<p>Proper scaling improves:</p>
<ul>
<li>training speed</li>
<li>numerical stability</li>
<li>convergence behavior</li>
</ul>
<hr />
<h1 id="standardization">Standardization</h1>
<p>Standardization transforms data so that:</p>
<ul>
<li>mean becomes 0</li>
<li>standard deviation becomes 1</li>
</ul>
<p>After transformation:</p>
<ul>
<li>most values fall roughly between -3 and +3</li>
</ul>
<p>Standardization centers the distribution and keeps relative distances
between samples.</p>
<p>It works well when:</p>
<ul>
<li>data is approximately normally distributed</li>
<li>features have different units</li>
<li>inputs are</li>
</ul>
</body>
</html>
