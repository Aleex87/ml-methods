<!DOCTYPE html>
<html>
<head>
  <meta charset="UTF-8">
  <title> normalization vs standardization.Value.ToUpper() ormalization
normalization vs standardization.Value.ToUpper() s normalization vs
standardization.Value.ToUpper() tandardization</title>
  <link rel="stylesheet" href="../../assets/css/style.css">
</head>

<body>

  <div class="page-banner">
    <h1> normalization vs standardization.Value.ToUpper() ormalization
normalization vs standardization.Value.ToUpper() s normalization vs
standardization.Value.ToUpper() tandardization</h1>
    <p>Foundations topic</p>
  </div>

  <div class="container">
    <div class="section-card">
      <h1
      id="normalization-vs-standardization-in-deep-learning">Normalization
      vs Standardization in Deep Learning</h1>
      <p>In deep learning, proper data scaling is critical.</p>
      <p>Neural networks are trained using gradient descent. If input
      features have very different scales, training can become unstable
      or inefficient.</p>
      <p>Normalization and standardization are two common preprocessing
      techniques.</p>
      <hr />
      <h1 id="why-scaling-matters-in-neural-networks">Why Scaling
      Matters in Neural Networks</h1>
      <p>Neural networks rely on:</p>
      <ul>
      <li>weighted sums</li>
      <li>activation functions</li>
      <li>gradient-based optimization</li>
      </ul>
      <p>If input values are too large:</p>
      <ul>
      <li>gradients may explode</li>
      <li>updates become unstable</li>
      </ul>
      <p>If input values are too small:</p>
      <ul>
      <li>gradients may vanish</li>
      <li>learning becomes slow</li>
      </ul>
      <p>Proper scaling improves:</p>
      <ul>
      <li>training speed</li>
      <li>numerical stability</li>
      <li>convergence behavior</li>
      </ul>
      <hr />
      <h1 id="standardization">Standardization</h1>
      <p>Standardization transforms data so that:</p>
      <ul>
      <li>mean becomes 0</li>
      <li>standard deviation becomes 1</li>
      </ul>
      <p>After transformation:</p>
      <ul>
      <li>most values fall roughly between -3 and +3</li>
      </ul>
      <p>Standardization centers the distribution and keeps relative
      distances between samples.</p>
      <p>It works well when:</p>
      <ul>
      <li>data is approximately normally distributed</li>
      <li>features have different units</li>
      <li>inputs are</li>
      </ul>
    </div>

    <p style="text-align:center; margin-top:60px;">
      <a href="../index.html">‚Üê Back</a>
    </p>
  </div>

</body>
</html>