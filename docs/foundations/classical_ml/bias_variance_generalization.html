<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>bias_variance_generalization</title>
  <style>
    /* Default styles provided by pandoc.
    ** See https://pandoc.org/MANUAL.html#variables-for-html for config info.
    */
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    .display.math{display: block; text-align: center; margin: 0.5rem auto;}
  </style>
  <link rel="stylesheet" href="../../assets/css/style.css" />
</head>
<body>
<h1 id="bias-variance-and-generalization">Bias, Variance, and
Generalization</h1>
<p>In machine learning, the goal is to learn a function that
approximates the true relationship between inputs (X) and outputs
(y).</p>
<p>However, every model makes assumptions.</p>
<p>Prediction error can be decomposed into three components:</p>
<ul>
<li>Bias</li>
<li>Variance</li>
<li>Irreducible error</li>
</ul>
<p>Understanding these components explains why models underfit or
overfit.</p>
<hr />
<h1 id="the-true-function">The True Function</h1>
<p>Assume there exists an unknown true function:</p>
<pre><code>y = f(x) + ε</code></pre>
<p>Where: - f(x) is the real underlying relationship - ε is random noise
(irreducible error)</p>
<p>A model tries to approximate f(x) with an estimated function:</p>
<pre><code>ŷ = g(x)</code></pre>
<p>The difference between f(x) and g(x) is where bias and variance
arise.</p>
<hr />
<h1 id="bias">Bias</h1>
<p>Bias measures how far the average model prediction is from the true
function.</p>
<p>Formally:</p>
<pre><code>Bias(x) = E[g(x)] − f(x)</code></pre>
<p>Bias represents systematic error.</p>
<p>Geometrically:</p>
<ul>
<li>If the true relationship is curved</li>
<li>But the model is linear</li>
</ul>
<p>The model cannot bend to match the curve.</p>
<p>It will consistently miss the true pattern, even if trained on many
datasets.</p>
<p>High bias means:</p>
<ul>
<li>The model is too rigid</li>
<li>The hypothesis space is too limited</li>
<li>Important structure cannot be captured</li>
</ul>
<p>This leads to underfitting.</p>
<hr />
<h1 id="variance">Variance</h1>
<p>Variance measures how much the model prediction changes when trained
on different datasets.</p>
<p>Formally:</p>
<pre><code>Variance(x) = E[(g(x) − E[g(x)])²]</code></pre>
<p>Geometrically:</p>
<ul>
<li>If we train the same complex model on slightly different data,</li>
<li>The learned function may change significantly.</li>
</ul>
<p>High variance means:</p>
<ul>
<li>The model is too flexible</li>
<li>It adapts strongly to noise</li>
<li>It memorizes training samples</li>
</ul>
<p>This leads to overfitting.</p>
<hr />
<h1 id="error-decomposition">Error Decomposition</h1>
<p>For a given input x, expected prediction error can be decomposed
as:</p>
<pre><code>Total Error = Bias² + Variance + Irreducible Error</code></pre>
<p>Irreducible error comes from noise ε and cannot be eliminated.</p>
<p>The model can only control bias and variance.</p>
<hr />
<h1 id="geometric-interpretation">Geometric Interpretation</h1>
<p>Consider fitting points in 2D space.</p>
<p>Case 1: Linear model for nonlinear data - The fitted line cannot
follow the curve - Systematic deviation appears → High Bias</p>
<p>Case 2: Very high-degree polynomial - The curve passes through every
point - Small fluctuations in data drastically change the shape → High
Variance</p>
<p>The ideal model:</p>
<ul>
<li>Follows the true pattern</li>
<li>Ignores random noise</li>
<li>Remains stable across datasets</li>
</ul>
<hr />
<h1 id="biasvariance-tradeoff">Bias–Variance Tradeoff</h1>
<p>Model complexity affects both components:</p>
<ul>
<li>Increasing complexity ↓ Bias</li>
<li>Increasing complexity ↑ Variance</li>
</ul>
<p>There is no free improvement.</p>
<p>The goal is to find a balance that minimizes total error.</p>
<hr />
<h1 id="connection-to-overfitting-and-underfitting">Connection to
Overfitting and Underfitting</h1>
<p>Underfitting: - High Bias - Low Variance - Poor training
performance</p>
<p>Overfitting: - Low Bias - High Variance - Large gap between training
and validation performance</p>
<hr />
<h1 id="why-this-matters">Why This Matters</h1>
<p>Bias and variance explain:</p>
<ul>
<li>Why adding parameters sometimes improves performance</li>
<li>Why adding too many parameters can hurt performance</li>
<li>Why regularization works</li>
<li>Why more data reduces variance</li>
</ul>
<p>They are central to understanding model behavior.</p>
</body>
</html>
