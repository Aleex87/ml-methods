{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2aa9d07",
   "metadata": {},
   "source": [
    "# Deep Learning – Classification (scikit-learn)\n",
    "\n",
    "This notebook is part of the **ML-Methods** project.\n",
    "\n",
    "It introduces **Deep Learning for supervised classification**\n",
    "using the scikit-learn implementation of neural networks.\n",
    "\n",
    "As with the other classification notebooks,\n",
    "the first sections focus on data preparation\n",
    "and are intentionally repeated.\n",
    "\n",
    "This ensures consistency across models\n",
    "and allows fair comparison of results.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## Notebook Roadmap (standard ML-Methods)\n",
    "\n",
    "1. Project setup and common pipeline  \n",
    "2. Dataset loading  \n",
    "3. Train-test split  \n",
    "4. Feature scaling (why we do it)  \n",
    "\n",
    "----------------------------------\n",
    "\n",
    "5. What is this model? (Intuition)  \n",
    "6. Model training  \n",
    "7. Model behavior and key parameters  \n",
    "8. Predictions  \n",
    "9. Model evaluation  \n",
    "10. When to use it and when not to  \n",
    "11. Model persistence  \n",
    "12. Mathematical formulation (deep dive)  \n",
    "13. Final summary – Code only  \n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## How this notebook should be read\n",
    "\n",
    "This notebook is designed to be read **top to bottom**.\n",
    "\n",
    "Before every code cell, you will find a short explanation describing:\n",
    "- what we are about to do\n",
    "- why this step is necessary\n",
    "- how it fits into the overall process\n",
    "\n",
    "The goal is not just to run the code,\n",
    "but to understand how **deep learning models**\n",
    "fit into the supervised learning pipeline\n",
    "and how they differ from classical machine learning methods.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## What is Deep Learning (in this context)?\n",
    "\n",
    "Deep Learning refers to a class of models\n",
    "based on **neural networks with multiple layers**.\n",
    "\n",
    "These models are designed to:\n",
    "- learn complex, non-linear relationships\n",
    "- automatically build internal representations\n",
    "- go beyond hand-crafted features\n",
    "\n",
    "In this notebook, we focus on:\n",
    "**Deep Learning for tabular data**\n",
    "using a **Multi-Layer Perceptron (MLP)**.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## Why use scikit-learn for Deep Learning?\n",
    "\n",
    "scikit-learn provides a **high-level abstraction**\n",
    "for neural networks through `MLPClassifier`.\n",
    "\n",
    "This allows us to:\n",
    "- focus on concepts rather than low-level details\n",
    "- reuse the same pipeline as classical ML models\n",
    "- understand *what* deep learning does\n",
    "  before learning *how* it is implemented internally\n",
    "\n",
    "This notebook acts as a **conceptual bridge**\n",
    "between classical machine learning\n",
    "and full deep learning frameworks\n",
    "such as PyTorch and TensorFlow.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## What you should expect from the results\n",
    "\n",
    "With Deep Learning (scikit-learn), you should expect:\n",
    "\n",
    "- non-linear decision boundaries\n",
    "- improved performance on complex patterns\n",
    "- higher sensitivity to feature scaling\n",
    "- longer training times compared to linear models\n",
    "\n",
    "However:\n",
    "- interpretability is lower\n",
    "- hyperparameter tuning becomes more important\n",
    "- the model behaves as a black box\n",
    "\n",
    "-----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b53c28",
   "metadata": {},
   "source": [
    "## 1. Project setup and common pipeline \n",
    "\n",
    "In this section we set up the common pipeline\n",
    "used across classification models in this project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a91e71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Common imports used across classification models\n",
    "# ====================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe34c3f9",
   "metadata": {},
   "source": [
    "-----------------------------------------------------\n",
    "## 2. Dataset loading\n",
    "\n",
    "In this section we load the dataset\n",
    "used for the Deep Learning classification task.\n",
    "\n",
    "We intentionally use the **same dataset**\n",
    "adopted in previous classification notebooks.\n",
    "\n",
    "This allows:\n",
    "- direct comparison with classical ML models\n",
    "- isolation of the effect of the model choice\n",
    "- consistent evaluation across methods\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75fbefd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Dataset loading\n",
    "# ====================================\n",
    "\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6effc172",
   "metadata": {},
   "source": [
    "### Inputs and target\n",
    "\n",
    "The dataset is composed of:\n",
    "- input features \\( X \\)\n",
    "- target labels \\( y \\)\n",
    "\n",
    "This is a **binary classification problem**,\n",
    "where each sample belongs to one of two classes.\n",
    "\n",
    "\n",
    "\n",
    "### Why dataset consistency matters\n",
    "\n",
    "Using the same dataset across models helps us:\n",
    "- understand performance differences\n",
    "- evaluate model complexity vs gains\n",
    "- avoid misleading conclusions\n",
    "\n",
    "Any improvement in results\n",
    "can be attributed to the model itself,\n",
    "not to changes in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c13cd70",
   "metadata": {},
   "source": [
    "____________________________________________________________________________\n",
    "## 3. Train-test split\n",
    "\n",
    "In this section we split the dataset\n",
    "into training and test sets.\n",
    "\n",
    "This step is essential to evaluate\n",
    "how well the Deep Learning model generalizes\n",
    "to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a9be9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Train-test split\n",
    "# ====================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb5a253",
   "metadata": {},
   "source": [
    "### What we have after this step\n",
    "\n",
    "After the split:\n",
    "- the **training set** is used to learn model parameters\n",
    "- the **test set** is kept completely unseen\n",
    "- evaluation will reflect real-world performance\n",
    "\n",
    "An 80 / 20 split is a common default\n",
    "for medium-sized datasets.\n",
    "\n",
    "In the next step,\n",
    "we will apply **feature scaling**.\n",
    "\n",
    "For Deep Learning models,\n",
    "this step is **mandatory**,\n",
    "not optional.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04014a14",
   "metadata": {},
   "source": [
    "<!-- -----------------------------------------------------\n",
    "## 2. Dataset loading\n",
    "\n",
    "In this section we load the dataset\n",
    "used for the Deep Learning classification task.\n",
    "\n",
    "We intentionally use the **same dataset**\n",
    "adopted in previous classification notebooks.\n",
    "\n",
    "This allows:\n",
    "- direct comparison with classical ML models\n",
    "- isolation of the effect of the model choice\n",
    "- consistent evaluation across methods\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "### Inputs and target\n",
    "\n",
    "The dataset is composed of:\n",
    "- input features \\( X \\)\n",
    "- target labels \\( y \\)\n",
    "\n",
    "This is a **binary classification problem**,\n",
    "where each sample belongs to one of two classes.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "### Why dataset consistency matters\n",
    "\n",
    "Using the same dataset across models helps us:\n",
    "- understand performance differences\n",
    "- evaluate model complexity vs gains\n",
    "- avoid misleading conclusions\n",
    "\n",
    "Any improvement in results\n",
    "can be attributed to the model itself,\n",
    "not to changes in the data.\n",
    "\n",
    "----------------------------------------------------- -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2256a3f",
   "metadata": {},
   "source": [
    "__________________________________________________\n",
    "\n",
    "## 4. Feature scaling (why we do it)\n",
    "\n",
    "In this section we apply feature scaling\n",
    "to the input data.\n",
    "\n",
    "For Deep Learning models,\n",
    "feature scaling is **not optional**.\n",
    "\n",
    "Neural networks are trained using\n",
    "gradient-based optimization methods,\n",
    "which are highly sensitive to feature scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a461bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Feature scaling\n",
    "# ====================================\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45025d10",
   "metadata": {},
   "source": [
    "### Why scaling is mandatory for Deep Learning\n",
    "\n",
    "Without proper scaling:\n",
    "- gradients may explode or vanish\n",
    "- optimization becomes unstable\n",
    "- training can be very slow or fail entirely\n",
    "\n",
    "By scaling the features:\n",
    "- all inputs contribute on a comparable scale\n",
    "- gradient descent converges faster\n",
    "- training becomes more stable and reliable\n",
    "\n",
    "### Important rule\n",
    "\n",
    "The scaler is:\n",
    "- **fitted only on the training data**\n",
    "- **applied to the test data**\n",
    "\n",
    "This prevents data leakage\n",
    "and ensures a fair evaluation.\n",
    "\n",
    "At this point,\n",
    "the data is fully prepared\n",
    "for training a neural network classifier.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
