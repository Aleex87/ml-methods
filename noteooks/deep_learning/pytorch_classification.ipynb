{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fbd26f1",
   "metadata": {},
   "source": [
    "# Deep Learning – Classification (PyTorch)\n",
    "\n",
    "This notebook is part of the **ML-Methods** project.\n",
    "\n",
    "It introduces **Deep Learning for supervised classification**\n",
    "using **PyTorch**, a low-level and flexible deep learning framework.\n",
    "\n",
    "As with the other classification notebooks,\n",
    "the first sections focus on data preparation\n",
    "and are intentionally repeated.\n",
    "\n",
    "This ensures consistency across models\n",
    "and allows fair comparison of results.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## Notebook Roadmap (standard ML-Methods)\n",
    "\n",
    "1. Project setup and common pipeline  \n",
    "2. Dataset loading  \n",
    "3. Train-test split  \n",
    "4. Feature scaling (why we do it)  \n",
    "\n",
    "----------------------------------\n",
    "\n",
    "5. What is this model? (Intuition)  \n",
    "6. Model training  \n",
    "7. Model behavior and key parameters  \n",
    "8. Predictions  \n",
    "9. Model evaluation  \n",
    "10. When to use it and when not to  \n",
    "11. Model persistence  \n",
    "12. Mathematical formulation (deep dive)  \n",
    "13. Final summary – Code only  \n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## How this notebook should be read\n",
    "\n",
    "This notebook is designed to be read **top to bottom**.\n",
    "\n",
    "Before every code cell, you will find a short explanation describing:\n",
    "- what we are about to do\n",
    "- why this step is necessary\n",
    "- how it fits into the overall process\n",
    "\n",
    "Compared to scikit-learn,\n",
    "this notebook exposes **more internal details**\n",
    "of how a Deep Learning model is trained.\n",
    "\n",
    "The goal is not only to run the code,\n",
    "but to understand **what happens during training**\n",
    "and how neural networks learn step by step.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## What is Deep Learning (in this context)?\n",
    "\n",
    "Deep Learning refers to a class of models\n",
    "based on **neural networks with multiple layers**.\n",
    "\n",
    "These models are designed to:\n",
    "- learn complex, non-linear relationships\n",
    "- build internal representations of the data\n",
    "- improve performance as data complexity increases\n",
    "\n",
    "In this notebook, we focus on:\n",
    "**Deep Learning for tabular classification**\n",
    "using fully connected neural networks.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## Why PyTorch?\n",
    "\n",
    "PyTorch is a **low-level deep learning framework**\n",
    "that provides explicit control over:\n",
    "\n",
    "- model architecture\n",
    "- forward pass\n",
    "- loss computation\n",
    "- backpropagation\n",
    "- parameter updates\n",
    "\n",
    "Unlike scikit-learn:\n",
    "- nothing is hidden\n",
    "- every step must be defined explicitly\n",
    "\n",
    "This makes PyTorch ideal for:\n",
    "- learning how neural networks actually work\n",
    "- understanding gradient-based optimization\n",
    "- experimenting with custom architectures\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## Execution model: eager execution\n",
    "\n",
    "PyTorch uses **eager execution** by default.\n",
    "\n",
    "This means:\n",
    "- operations are executed immediately\n",
    "- tensors behave like regular Python objects\n",
    "- debugging is straightforward\n",
    "\n",
    "Eager execution makes PyTorch:\n",
    "- intuitive to learn\n",
    "- flexible to experiment with\n",
    "- closer to the mathematical description of the model\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## What you should expect from the results\n",
    "\n",
    "With Deep Learning (PyTorch), you should expect:\n",
    "\n",
    "- non-linear decision boundaries\n",
    "- strong performance on complex data\n",
    "- behavior similar to scikit-learn neural networks\n",
    "- higher transparency during training\n",
    "\n",
    "However:\n",
    "- more code is required\n",
    "- implementation errors are easier to make\n",
    "- careful design is necessary\n",
    "\n",
    "-----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa43a78",
   "metadata": {},
   "source": [
    "## 1. Project setup and common pipeline\n",
    "\n",
    "In this section we set up the common pipeline\n",
    "used across classification models in this project.\n",
    "\n",
    "Although this notebook uses **PyTorch**,\n",
    "the overall workflow remains identical\n",
    "to the scikit-learn Deep Learning notebook.\n",
    "\n",
    "This allows us to:\n",
    "- reuse the same data preparation steps\n",
    "- compare models fairly\n",
    "- isolate the effect of the framework choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2856c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports used across classification models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====================================\n",
    "# PyTorch imports\n",
    "# ====================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d10527",
   "metadata": {},
   "source": [
    "### What changes with PyTorch\n",
    "\n",
    "Compared to scikit-learn:\n",
    "- the pipeline structure remains the same\n",
    "- data preparation and evaluation stay unchanged\n",
    "- only the model implementation differs\n",
    "\n",
    "With PyTorch, we explicitly define:\n",
    "- how the model processes the input\n",
    "- how the loss is computed\n",
    "- how parameters are updated\n",
    "\n",
    "Nothing is hidden.\n",
    "\n",
    "Every step of the learning process\n",
    "is written manually in code.\n",
    "\n",
    "This makes PyTorch ideal\n",
    "for understanding what neural networks\n",
    "are actually doing during training.\n",
    "\n",
    "In the next section,\n",
    "we will load the dataset\n",
    "and prepare it for PyTorch training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993fc4d9",
   "metadata": {},
   "source": [
    "____________\n",
    "## 2. Dataset loading\n",
    "\n",
    "In this section we load the dataset\n",
    "used for the Deep Learning classification task.\n",
    "\n",
    "We intentionally use the **same dataset**\n",
    "adopted in previous classification notebooks.\n",
    "\n",
    "This ensures:\n",
    "- direct comparison with classical ML models\n",
    "- fair comparison across deep learning frameworks\n",
    "- focus on implementation differences, not on data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bceaead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Dataset loading\n",
    "# ====================================\n",
    "\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02657345",
   "metadata": {},
   "source": [
    "### What we have after this step\n",
    "\n",
    "- `X` contains the input features\n",
    "- `y` contains the target labels\n",
    "\n",
    "This is a **binary classification problem**,\n",
    "where each sample belongs to one of two classes.\n",
    "\n",
    "At this stage:\n",
    "- data is still in NumPy / pandas format\n",
    "- this is intentional for consistency\n",
    "- conversion to PyTorch tensors will happen later\n",
    "\n",
    "In the next section,\n",
    "we will split the dataset\n",
    "into training and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a3470",
   "metadata": {},
   "source": [
    "_______________\n",
    "## 3. Train-test split\n",
    "\n",
    "In this section we split the dataset\n",
    "into training and test sets.\n",
    "\n",
    "This step allows us to evaluate\n",
    "how well the neural network generalizes\n",
    "to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a4868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Train-test split\n",
    "# ====================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b9d71",
   "metadata": {},
   "source": [
    "### What we have after this step\n",
    "\n",
    "After splitting the data:\n",
    "- the training set is used to learn model parameters\n",
    "- the test set is kept completely unseen\n",
    "- evaluation reflects real-world performance\n",
    "\n",
    "An 80 / 20 split is a common default\n",
    "for medium-sized datasets.\n",
    "\n",
    "In the next section,\n",
    "we will apply **feature scaling**.\n",
    "\n",
    "For Deep Learning models,\n",
    "this step is **mandatory**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da373b1",
   "metadata": {},
   "source": [
    "_____________\n",
    "## 4. Feature scaling (why we do it)\n",
    "\n",
    "In this section we apply feature scaling\n",
    "to the input data.\n",
    "\n",
    "For Deep Learning models,\n",
    "feature scaling is **mandatory**.\n",
    "\n",
    "Neural networks rely on gradient-based optimization,\n",
    "which is highly sensitive to feature scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6aaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21332c1",
   "metadata": {},
   "source": [
    "### Why we use standardization here\n",
    "\n",
    "We use **standardization** for feature scaling\n",
    "because neural networks are trained\n",
    "using gradient-based optimization.\n",
    "\n",
    "Standardization:\n",
    "- centers features around zero\n",
    "- ensures comparable feature variance\n",
    "- improves numerical stability during training\n",
    "\n",
    "This helps:\n",
    "- gradients behave more predictably\n",
    "- optimization converge faster\n",
    "- training remain stable across layers\n",
    "\n",
    "For Deep Learning models,\n",
    "this preprocessing choice is part of the model design,\n",
    "not just a data transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce83c1",
   "metadata": {},
   "source": [
    "### Why scaling is essential here\n",
    "\n",
    "Without proper scaling:\n",
    "- gradients may vanish or explode\n",
    "- optimization becomes unstable\n",
    "- training may converge very slowly or fail\n",
    "\n",
    "By scaling the features:\n",
    "- all inputs are brought to a comparable range\n",
    "- gradient descent becomes more stable\n",
    "- learning is faster and more reliable\n",
    "\n",
    "At this point:\n",
    "- data is numerically ready\n",
    "- but still in NumPy format\n",
    "\n",
    "In the next section,\n",
    "we will explain **what this model is**\n",
    "and how a neural network performs classification in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b87960",
   "metadata": {},
   "source": [
    "____________\n",
    "## 5. What is this model? (Deep Learning with PyTorch)\n",
    "\n",
    "Before writing any PyTorch code,\n",
    "it is important to understand\n",
    "**what the model does in practice**\n",
    "and **what we are manually controlling**.\n",
    "\n",
    "PyTorch does not hide the learning process.\n",
    "Everything that happens during training\n",
    "is explicitly written by us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdea9a",
   "metadata": {},
   "source": [
    "### What do we want to achieve?\n",
    "\n",
    "We want to build a model that:\n",
    "- receives a vector of input features\n",
    "- processes them through multiple transformations\n",
    "- outputs a class prediction\n",
    "\n",
    "Each input sample can be seen as:\n",
    "- a list of numerical measurements\n",
    "- describing a single object\n",
    "- represented as a vector\n",
    "\n",
    "The goal of the model is to learn\n",
    "how to transform this vector\n",
    "into the correct class label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29516b",
   "metadata": {},
   "source": [
    "### What does the model do, step by step?\n",
    "\n",
    "A neural network for classification performs\n",
    "the following operations:\n",
    "\n",
    "1. Take the input feature vector  \n",
    "2. Multiply it by learnable weights  \n",
    "3. Add a bias term  \n",
    "4. Apply a non-linear function  \n",
    "5. Repeat this process across multiple layers  \n",
    "6. Produce an output used to decide the class  \n",
    "\n",
    "Each step is simple.\n",
    "The power comes from repeating them\n",
    "many times in sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9232df74",
   "metadata": {},
   "source": [
    "### What is a neuron, technically?\n",
    "\n",
    "A neuron is a very simple computational unit.\n",
    "\n",
    "It answers one basic question:\n",
    "> *Is a specific pattern present in the input?*\n",
    "\n",
    "Technically, a neuron:\n",
    "- combines input features linearly\n",
    "- applies a non-linear transformation\n",
    "- outputs a single value\n",
    "\n",
    "During training,\n",
    "the neuron learns which patterns matter\n",
    "by adjusting its weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe54f78",
   "metadata": {},
   "source": [
    "### Why multiple layers?\n",
    "\n",
    "A single layer can only learn\n",
    "simple patterns.\n",
    "\n",
    "By stacking layers:\n",
    "- early layers learn basic feature combinations\n",
    "- deeper layers learn more abstract patterns\n",
    "- the final layer focuses on class separation\n",
    "\n",
    "Each layer builds on the output\n",
    "of the previous one.\n",
    "\n",
    "This is how the model gradually constructs\n",
    "a useful internal representation of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134d33c",
   "metadata": {},
   "source": [
    "### What makes PyTorch different here?\n",
    "\n",
    "With PyTorch:\n",
    "- we explicitly define the network structure\n",
    "- we manually control the training loop\n",
    "- we decide how loss is computed\n",
    "- we decide how parameters are updated\n",
    "\n",
    "Nothing is automatic or hidden.\n",
    "\n",
    "This allows us to:\n",
    "- see how predictions are produced\n",
    "- understand how errors drive learning\n",
    "- connect code directly to theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076cb1de",
   "metadata": {},
   "source": [
    "### How learning happens conceptually\n",
    "\n",
    "Learning follows a simple cycle:\n",
    "\n",
    "1. The model makes a prediction  \n",
    "2. The prediction is compared to the true label  \n",
    "3. An error value is computed  \n",
    "4. The model parameters are adjusted  \n",
    "5. The process repeats  \n",
    "\n",
    "Each iteration slightly improves\n",
    "the alignment between predictions\n",
    "and true labels.\n",
    "\n",
    "This gradual improvement\n",
    "is what we call training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf5e4a",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "A PyTorch neural network classifier:\n",
    "- processes data step by step\n",
    "- learns by repeatedly correcting its mistakes\n",
    "- builds internal representations through layers\n",
    "- requires explicit definition of every step\n",
    "\n",
    "PyTorch forces us to understand\n",
    "**how learning actually happens**,\n",
    "not just what the final result is.\n",
    "\n",
    "In the next section,\n",
    "we will implement this process manually\n",
    "by defining the model\n",
    "and writing the training loop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde3ee3",
   "metadata": {},
   "source": [
    "__________________________\n",
    "## 6. Model training (PyTorch)\n",
    "\n",
    "In this section we define and train\n",
    "a neural network classifier using PyTorch.\n",
    "\n",
    "Unlike scikit-learn,\n",
    "PyTorch requires us to explicitly write:\n",
    "\n",
    "- the model architecture\n",
    "- the loss function\n",
    "- the optimizer\n",
    "- the training loop\n",
    "\n",
    "This section shows\n",
    "what “training a neural network” really means in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efa8e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Convert data to PyTorch tensors\n",
    "# ====================================\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a69f8ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Define the neural network model\n",
    "# ====================================\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.out = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = NeuralNetwork(input_dim=X_train_tensor.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a43fdda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Loss function and optimizer\n",
    "# ====================================\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24e30734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.6057\n",
      "Epoch [20/50], Loss: 0.4540\n",
      "Epoch [30/50], Loss: 0.2879\n",
      "Epoch [40/50], Loss: 0.1770\n",
      "Epoch [50/50], Loss: 0.1231\n"
     ]
    }
   ],
   "source": [
    "# ====================================\n",
    "# Training loop\n",
    "# ====================================\n",
    "\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    loss = criterion(outputs, y_train_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2612087c",
   "metadata": {},
   "source": [
    "### What we just did (step by step, in detail)\n",
    "\n",
    "In this section we manually implemented\n",
    "the entire training process of a neural network.\n",
    "\n",
    "Nothing was automatic.\n",
    "Every step was explicitly written.\n",
    "\n",
    "Let’s break it down carefully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9604553",
   "metadata": {},
   "source": [
    "#### 1. Converting data to PyTorch tensors\n",
    "\n",
    "PyTorch does not operate on NumPy arrays or pandas objects.\n",
    "\n",
    "For this reason:\n",
    "- input features were converted to `float32` tensors\n",
    "- target labels were converted to integer tensors\n",
    "\n",
    "This step is purely technical,\n",
    "but it is mandatory:\n",
    "without tensors, PyTorch cannot compute gradients\n",
    "or perform optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d262a1",
   "metadata": {},
   "source": [
    "#### 2. Defining the neural network architecture\n",
    "\n",
    "We defined a neural network by creating a class\n",
    "that inherits from `nn.Module`.\n",
    "\n",
    "Inside this class:\n",
    "- each layer is explicitly declared\n",
    "- each layer has its own learnable parameters\n",
    "- weights and biases are initialized automatically\n",
    "\n",
    "The architecture defines:\n",
    "- how many transformations are applied\n",
    "- how information flows from input to output\n",
    "\n",
    "At this stage, the model knows:\n",
    "**how to process data**,\n",
    "but it does not yet know **how to solve the problem**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29f2fa2",
   "metadata": {},
   "source": [
    "#### 3. Defining the forward pass\n",
    "\n",
    "The `forward` method specifies\n",
    "what happens when data flows through the network.\n",
    "\n",
    "In practice:\n",
    "- input data enters the first layer\n",
    "- linear transformations are applied\n",
    "- non-linear activations are applied\n",
    "- this process repeats across layers\n",
    "- the final layer produces raw class scores\n",
    "\n",
    "The forward pass defines the **computation graph**\n",
    "that PyTorch uses to track operations\n",
    "and compute gradients automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c7f89",
   "metadata": {},
   "source": [
    "#### 4. Choosing the loss function\n",
    "\n",
    "A neural network cannot learn\n",
    "without a way to measure its mistakes.\n",
    "\n",
    "The loss function:\n",
    "- compares model predictions to true labels\n",
    "- outputs a single scalar value\n",
    "- represents how wrong the model is\n",
    "\n",
    "`CrossEntropyLoss` is used because:\n",
    "- this is a classification problem\n",
    "- the output consists of class scores\n",
    "- it combines softmax and log-loss internally\n",
    "\n",
    "The loss is the signal\n",
    "that drives learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f093489f",
   "metadata": {},
   "source": [
    "#### 5. Choosing the optimizer\n",
    "\n",
    "The optimizer defines **how the model learns**.\n",
    "\n",
    "It takes:\n",
    "- the gradients of the loss\n",
    "- the current model parameters\n",
    "\n",
    "and updates the parameters\n",
    "to reduce the error.\n",
    "\n",
    "Adam is used because:\n",
    "- it adapts learning rates automatically\n",
    "- it is stable in practice\n",
    "- it works well for most deep learning tasks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73324a4",
   "metadata": {},
   "source": [
    "#### 6. Writing the training loop\n",
    "\n",
    "The training loop is the heart of Deep Learning.\n",
    "\n",
    "For each iteration:\n",
    "1. Gradients are reset  \n",
    "2. The model makes predictions on training data  \n",
    "3. The loss is computed  \n",
    "4. Gradients are calculated via backpropagation  \n",
    "5. Parameters are updated  \n",
    "\n",
    "This process is repeated many times.\n",
    "\n",
    "Each iteration:\n",
    "- slightly reduces the error\n",
    "- slightly improves the model\n",
    "- moves predictions closer to true labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070f9fde",
   "metadata": {},
   "source": [
    "#### 7. What learning means in practice\n",
    "\n",
    "Learning does not happen all at once.\n",
    "\n",
    "Instead:\n",
    "- the model starts with random parameters\n",
    "- predictions are initially poor\n",
    "- each update corrects the model slightly\n",
    "- performance improves gradually\n",
    "\n",
    "This slow, iterative correction process\n",
    "is what makes neural networks powerful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4b66a0",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "Training a neural network in PyTorch means:\n",
    "\n",
    "- explicitly defining how data flows\n",
    "- explicitly measuring prediction error\n",
    "- explicitly updating model parameters\n",
    "- repeating this process many times\n",
    "\n",
    "PyTorch exposes the full learning mechanism.\n",
    "\n",
    "Nothing is hidden.\n",
    "\n",
    "Understanding this loop\n",
    "means understanding Deep Learning at its core.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e1be02",
   "metadata": {},
   "source": [
    "_______________________\n",
    "## 7. Model behavior and key parameters\n",
    "\n",
    "In this section we analyze how the PyTorch neural network behaves\n",
    "and which parameters most strongly influence its performance.\n",
    "\n",
    "Unlike classical models,\n",
    "the behavior of a neural network\n",
    "is not described by a single formula or coefficient.\n",
    "\n",
    "Instead, it emerges from:\n",
    "- architecture choices\n",
    "- activation functions\n",
    "- training dynamics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92450f3",
   "metadata": {},
   "source": [
    "### Architecture and model capacity\n",
    "\n",
    "The architecture defines:\n",
    "- how many layers the model has\n",
    "- how many neurons are in each layer\n",
    "\n",
    "In our case:\n",
    "\n",
    "- input layer → `input_dim` features\n",
    "- first hidden layer → 64 neurons\n",
    "- second hidden layer → 32 neurons\n",
    "- output layer → 2 neurons (one per class)\n",
    "\n",
    "More neurons and layers mean:\n",
    "- higher capacity\n",
    "- ability to learn more complex patterns\n",
    "\n",
    "However:\n",
    "- too much capacity increases overfitting\n",
    "- too little capacity leads to underfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10047ab",
   "metadata": {},
   "source": [
    "### Role of hidden layers\n",
    "\n",
    "Hidden layers allow the model\n",
    "to build intermediate representations.\n",
    "\n",
    "Conceptually:\n",
    "- early layers combine raw features\n",
    "- deeper layers combine patterns of patterns\n",
    "- the final layer separates classes\n",
    "\n",
    "Each layer transforms the data\n",
    "into a representation that is\n",
    "easier to classify than the previous one.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71368bf",
   "metadata": {},
   "source": [
    "### Activation functions and non-linearity\n",
    "\n",
    "ReLU is used after each hidden layer.\n",
    "\n",
    "Its role is to:\n",
    "- introduce non-linearity\n",
    "- allow complex decision boundaries\n",
    "- keep training stable\n",
    "\n",
    "Without activation functions:\n",
    "- the network would behave like a linear model\n",
    "- depth would provide no benefit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0089e945",
   "metadata": {},
   "source": [
    "### Optimization behavior\n",
    "\n",
    "The optimizer controls\n",
    "how the model learns from errors.\n",
    "\n",
    "Adam:\n",
    "- adapts learning rates automatically\n",
    "- handles noisy gradients well\n",
    "- works reliably across many problems\n",
    "\n",
    "The learning rate controls:\n",
    "- how fast the model changes\n",
    "- stability of training\n",
    "\n",
    "If the learning rate is too high:\n",
    "- training becomes unstable\n",
    "\n",
    "If it is too low:\n",
    "- training is very slow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1492f8",
   "metadata": {},
   "source": [
    "### Training dynamics\n",
    "\n",
    "During training:\n",
    "- the loss should gradually decrease\n",
    "- parameter updates become smaller over time\n",
    "- predictions become more consistent\n",
    "\n",
    "Learning is not instantaneous.\n",
    "The model improves through\n",
    "many small corrections.\n",
    "\n",
    "This iterative refinement\n",
    "is a defining characteristic\n",
    "of neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57778fe",
   "metadata": {},
   "source": [
    "### Generalization behavior\n",
    "\n",
    "Neural networks tend to:\n",
    "- fit training data very well\n",
    "- risk overfitting if not controlled\n",
    "\n",
    "Good generalization requires:\n",
    "- appropriate model size\n",
    "- enough training data\n",
    "- careful monitoring of performance\n",
    "\n",
    "Evaluation on unseen data\n",
    "is therefore essential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6399a0f",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "The behavior of a PyTorch neural network\n",
    "is determined by:\n",
    "- how the network is built\n",
    "- how non-linearity is introduced\n",
    "- how learning is controlled\n",
    "\n",
    "Understanding these elements\n",
    "allows us to reason about model performance,\n",
    "instead of treating the network as a black box.\n",
    "\n",
    "In the next section,\n",
    "we will use the trained model\n",
    "to generate predictions\n",
    "and analyze its outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c149bfb",
   "metadata": {},
   "source": [
    "__________________\n",
    "## 8. Predictions\n",
    "\n",
    "In this section we use the trained PyTorch model\n",
    "to generate predictions on unseen test data.\n",
    "\n",
    "As with other classification models,\n",
    "it is important to distinguish between:\n",
    "- raw model outputs\n",
    "- predicted class labels\n",
    "- predicted class probabilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "53e97775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Predictions with PyTorch\n",
    "# ====================================\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "\n",
    "    logits = model(X_test_tensor)\n",
    "\n",
    "    y_pred = torch.argmax(logits, dim=1)\n",
    "\n",
    "    y_pred_proba = torch.softmax(logits, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31880aed",
   "metadata": {},
   "source": [
    "### What the model outputs\n",
    "\n",
    "The PyTorch model produces **logits**:\n",
    "- raw scores\n",
    "- one value per class\n",
    "- not probabilities\n",
    "\n",
    "These values indicate\n",
    "how strongly the model favors each class.\n",
    "\n",
    "### From logits to class labels\n",
    "\n",
    "To obtain predicted class labels:\n",
    "- we select the index of the largest logit\n",
    "- this corresponds to the most likely class\n",
    "\n",
    "This is done using `argmax`.\n",
    "\n",
    "### From logits to probabilities\n",
    "\n",
    "To obtain probabilities:\n",
    "- we apply the softmax function\n",
    "- values are normalized to sum to 1\n",
    "\n",
    "Probabilities are useful for:\n",
    "- measuring confidence\n",
    "- analyzing borderline cases\n",
    "- setting custom decision thresholds\n",
    "\n",
    "### Important difference from training\n",
    "\n",
    "During training:\n",
    "- `CrossEntropyLoss` applies softmax internally\n",
    "- logits are passed directly to the loss\n",
    "\n",
    "During prediction:\n",
    "- we explicitly apply softmax\n",
    "- because we want interpretable probabilities\n",
    "\n",
    "At this point, we have:\n",
    "- predicted class labels\n",
    "- predicted class probabilities\n",
    "\n",
    "In the next section,\n",
    "we will evaluate these predictions\n",
    "using standard classification metrics.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
