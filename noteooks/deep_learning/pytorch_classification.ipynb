{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1fbd26f1",
   "metadata": {},
   "source": [
    "# Deep Learning – Classification (PyTorch)\n",
    "\n",
    "This notebook is part of the **ML-Methods** project.\n",
    "\n",
    "It introduces **Deep Learning for supervised classification**\n",
    "using **PyTorch**, a low-level and flexible deep learning framework.\n",
    "\n",
    "As with the other classification notebooks,\n",
    "the first sections focus on data preparation\n",
    "and are intentionally repeated.\n",
    "\n",
    "This ensures consistency across models\n",
    "and allows fair comparison of results.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## Notebook Roadmap (standard ML-Methods)\n",
    "\n",
    "1. Project setup and common pipeline  \n",
    "2. Dataset loading  \n",
    "3. Train-test split  \n",
    "4. Feature scaling (why we do it)  \n",
    "\n",
    "----------------------------------\n",
    "\n",
    "5. What is this model? (Intuition)  \n",
    "6. Model training  \n",
    "7. Model behavior and key parameters  \n",
    "8. Predictions  \n",
    "9. Model evaluation  \n",
    "10. When to use it and when not to  \n",
    "11. Model persistence  \n",
    "12. Mathematical formulation (deep dive)  \n",
    "13. Final summary – Code only  \n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## How this notebook should be read\n",
    "\n",
    "This notebook is designed to be read **top to bottom**.\n",
    "\n",
    "Before every code cell, you will find a short explanation describing:\n",
    "- what we are about to do\n",
    "- why this step is necessary\n",
    "- how it fits into the overall process\n",
    "\n",
    "Compared to scikit-learn,\n",
    "this notebook exposes **more internal details**\n",
    "of how a Deep Learning model is trained.\n",
    "\n",
    "The goal is not only to run the code,\n",
    "but to understand **what happens during training**\n",
    "and how neural networks learn step by step.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## What is Deep Learning (in this context)?\n",
    "\n",
    "Deep Learning refers to a class of models\n",
    "based on **neural networks with multiple layers**.\n",
    "\n",
    "These models are designed to:\n",
    "- learn complex, non-linear relationships\n",
    "- build internal representations of the data\n",
    "- improve performance as data complexity increases\n",
    "\n",
    "In this notebook, we focus on:\n",
    "**Deep Learning for tabular classification**\n",
    "using fully connected neural networks.\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## Why PyTorch?\n",
    "\n",
    "PyTorch is a **low-level deep learning framework**\n",
    "that provides explicit control over:\n",
    "\n",
    "- model architecture\n",
    "- forward pass\n",
    "- loss computation\n",
    "- backpropagation\n",
    "- parameter updates\n",
    "\n",
    "Unlike scikit-learn:\n",
    "- nothing is hidden\n",
    "- every step must be defined explicitly\n",
    "\n",
    "This makes PyTorch ideal for:\n",
    "- learning how neural networks actually work\n",
    "- understanding gradient-based optimization\n",
    "- experimenting with custom architectures\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## Execution model: eager execution\n",
    "\n",
    "PyTorch uses **eager execution** by default.\n",
    "\n",
    "This means:\n",
    "- operations are executed immediately\n",
    "- tensors behave like regular Python objects\n",
    "- debugging is straightforward\n",
    "\n",
    "Eager execution makes PyTorch:\n",
    "- intuitive to learn\n",
    "- flexible to experiment with\n",
    "- closer to the mathematical description of the model\n",
    "\n",
    "-----------------------------------------------------\n",
    "\n",
    "## What you should expect from the results\n",
    "\n",
    "With Deep Learning (PyTorch), you should expect:\n",
    "\n",
    "- non-linear decision boundaries\n",
    "- strong performance on complex data\n",
    "- behavior similar to scikit-learn neural networks\n",
    "- higher transparency during training\n",
    "\n",
    "However:\n",
    "- more code is required\n",
    "- implementation errors are easier to make\n",
    "- careful design is necessary\n",
    "\n",
    "-----------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa43a78",
   "metadata": {},
   "source": [
    "## 1. Project setup and common pipeline\n",
    "\n",
    "In this section we set up the common pipeline\n",
    "used across classification models in this project.\n",
    "\n",
    "Although this notebook uses **PyTorch**,\n",
    "the overall workflow remains identical\n",
    "to the scikit-learn Deep Learning notebook.\n",
    "\n",
    "This allows us to:\n",
    "- reuse the same data preparation steps\n",
    "- compare models fairly\n",
    "- isolate the effect of the framework choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2856c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports used across classification models\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ====================================\n",
    "# PyTorch imports\n",
    "# ====================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d10527",
   "metadata": {},
   "source": [
    "### What changes with PyTorch\n",
    "\n",
    "Compared to scikit-learn:\n",
    "- the pipeline structure remains the same\n",
    "- data preparation and evaluation stay unchanged\n",
    "- only the model implementation differs\n",
    "\n",
    "With PyTorch, we explicitly define:\n",
    "- how the model processes the input\n",
    "- how the loss is computed\n",
    "- how parameters are updated\n",
    "\n",
    "Nothing is hidden.\n",
    "\n",
    "Every step of the learning process\n",
    "is written manually in code.\n",
    "\n",
    "This makes PyTorch ideal\n",
    "for understanding what neural networks\n",
    "are actually doing during training.\n",
    "\n",
    "In the next section,\n",
    "we will load the dataset\n",
    "and prepare it for PyTorch training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993fc4d9",
   "metadata": {},
   "source": [
    "____________\n",
    "## 2. Dataset loading\n",
    "\n",
    "In this section we load the dataset\n",
    "used for the Deep Learning classification task.\n",
    "\n",
    "We intentionally use the **same dataset**\n",
    "adopted in previous classification notebooks.\n",
    "\n",
    "This ensures:\n",
    "- direct comparison with classical ML models\n",
    "- fair comparison across deep learning frameworks\n",
    "- focus on implementation differences, not on data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bceaead0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Dataset loading\n",
    "# ====================================\n",
    "\n",
    "data = load_breast_cancer(as_frame=True)\n",
    "\n",
    "X = data.data\n",
    "y = data.target\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02657345",
   "metadata": {},
   "source": [
    "### What we have after this step\n",
    "\n",
    "- `X` contains the input features\n",
    "- `y` contains the target labels\n",
    "\n",
    "This is a **binary classification problem**,\n",
    "where each sample belongs to one of two classes.\n",
    "\n",
    "At this stage:\n",
    "- data is still in NumPy / pandas format\n",
    "- this is intentional for consistency\n",
    "- conversion to PyTorch tensors will happen later\n",
    "\n",
    "In the next section,\n",
    "we will split the dataset\n",
    "into training and test sets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487a3470",
   "metadata": {},
   "source": [
    "_______________\n",
    "## 3. Train-test split\n",
    "\n",
    "In this section we split the dataset\n",
    "into training and test sets.\n",
    "\n",
    "This step allows us to evaluate\n",
    "how well the neural network generalizes\n",
    "to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a4868d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Train-test split\n",
    "# ====================================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b9d71",
   "metadata": {},
   "source": [
    "### What we have after this step\n",
    "\n",
    "After splitting the data:\n",
    "- the training set is used to learn model parameters\n",
    "- the test set is kept completely unseen\n",
    "- evaluation reflects real-world performance\n",
    "\n",
    "An 80 / 20 split is a common default\n",
    "for medium-sized datasets.\n",
    "\n",
    "In the next section,\n",
    "we will apply **feature scaling**.\n",
    "\n",
    "For Deep Learning models,\n",
    "this step is **mandatory**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2da373b1",
   "metadata": {},
   "source": [
    "_____________\n",
    "## 4. Feature scaling (why we do it)\n",
    "\n",
    "In this section we apply feature scaling\n",
    "to the input data.\n",
    "\n",
    "For Deep Learning models,\n",
    "feature scaling is **mandatory**.\n",
    "\n",
    "Neural networks rely on gradient-based optimization,\n",
    "which is highly sensitive to feature scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d6aaac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21332c1",
   "metadata": {},
   "source": [
    "### Why we use standardization here\n",
    "\n",
    "We use **standardization** for feature scaling\n",
    "because neural networks are trained\n",
    "using gradient-based optimization.\n",
    "\n",
    "Standardization:\n",
    "- centers features around zero\n",
    "- ensures comparable feature variance\n",
    "- improves numerical stability during training\n",
    "\n",
    "This helps:\n",
    "- gradients behave more predictably\n",
    "- optimization converge faster\n",
    "- training remain stable across layers\n",
    "\n",
    "For Deep Learning models,\n",
    "this preprocessing choice is part of the model design,\n",
    "not just a data transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ce83c1",
   "metadata": {},
   "source": [
    "### Why scaling is essential here\n",
    "\n",
    "Without proper scaling:\n",
    "- gradients may vanish or explode\n",
    "- optimization becomes unstable\n",
    "- training may converge very slowly or fail\n",
    "\n",
    "By scaling the features:\n",
    "- all inputs are brought to a comparable range\n",
    "- gradient descent becomes more stable\n",
    "- learning is faster and more reliable\n",
    "\n",
    "At this point:\n",
    "- data is numerically ready\n",
    "- but still in NumPy format\n",
    "\n",
    "In the next section,\n",
    "we will explain **what this model is**\n",
    "and how a neural network performs classification in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b87960",
   "metadata": {},
   "source": [
    "____________\n",
    "## 5. What is this model? (Deep Learning with PyTorch)\n",
    "\n",
    "Before writing any PyTorch code,\n",
    "it is important to understand\n",
    "**what the model does in practice**\n",
    "and **what we are manually controlling**.\n",
    "\n",
    "PyTorch does not hide the learning process.\n",
    "Everything that happens during training\n",
    "is explicitly written by us.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cdea9a",
   "metadata": {},
   "source": [
    "### What do we want to achieve?\n",
    "\n",
    "We want to build a model that:\n",
    "- receives a vector of input features\n",
    "- processes them through multiple transformations\n",
    "- outputs a class prediction\n",
    "\n",
    "Each input sample can be seen as:\n",
    "- a list of numerical measurements\n",
    "- describing a single object\n",
    "- represented as a vector\n",
    "\n",
    "The goal of the model is to learn\n",
    "how to transform this vector\n",
    "into the correct class label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f29516b",
   "metadata": {},
   "source": [
    "### What does the model do, step by step?\n",
    "\n",
    "A neural network for classification performs\n",
    "the following operations:\n",
    "\n",
    "1. Take the input feature vector  \n",
    "2. Multiply it by learnable weights  \n",
    "3. Add a bias term  \n",
    "4. Apply a non-linear function  \n",
    "5. Repeat this process across multiple layers  \n",
    "6. Produce an output used to decide the class  \n",
    "\n",
    "Each step is simple.\n",
    "The power comes from repeating them\n",
    "many times in sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9232df74",
   "metadata": {},
   "source": [
    "### What is a neuron, technically?\n",
    "\n",
    "A neuron is a very simple computational unit.\n",
    "\n",
    "It answers one basic question:\n",
    "> *Is a specific pattern present in the input?*\n",
    "\n",
    "Technically, a neuron:\n",
    "- combines input features linearly\n",
    "- applies a non-linear transformation\n",
    "- outputs a single value\n",
    "\n",
    "During training,\n",
    "the neuron learns which patterns matter\n",
    "by adjusting its weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe54f78",
   "metadata": {},
   "source": [
    "### Why multiple layers?\n",
    "\n",
    "A single layer can only learn\n",
    "simple patterns.\n",
    "\n",
    "By stacking layers:\n",
    "- early layers learn basic feature combinations\n",
    "- deeper layers learn more abstract patterns\n",
    "- the final layer focuses on class separation\n",
    "\n",
    "Each layer builds on the output\n",
    "of the previous one.\n",
    "\n",
    "This is how the model gradually constructs\n",
    "a useful internal representation of the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2134d33c",
   "metadata": {},
   "source": [
    "### What makes PyTorch different here?\n",
    "\n",
    "With PyTorch:\n",
    "- we explicitly define the network structure\n",
    "- we manually control the training loop\n",
    "- we decide how loss is computed\n",
    "- we decide how parameters are updated\n",
    "\n",
    "Nothing is automatic or hidden.\n",
    "\n",
    "This allows us to:\n",
    "- see how predictions are produced\n",
    "- understand how errors drive learning\n",
    "- connect code directly to theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076cb1de",
   "metadata": {},
   "source": [
    "### How learning happens conceptually\n",
    "\n",
    "Learning follows a simple cycle:\n",
    "\n",
    "1. The model makes a prediction  \n",
    "2. The prediction is compared to the true label  \n",
    "3. An error value is computed  \n",
    "4. The model parameters are adjusted  \n",
    "5. The process repeats  \n",
    "\n",
    "Each iteration slightly improves\n",
    "the alignment between predictions\n",
    "and true labels.\n",
    "\n",
    "This gradual improvement\n",
    "is what we call training.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf5e4a",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "\n",
    "A PyTorch neural network classifier:\n",
    "- processes data step by step\n",
    "- learns by repeatedly correcting its mistakes\n",
    "- builds internal representations through layers\n",
    "- requires explicit definition of every step\n",
    "\n",
    "PyTorch forces us to understand\n",
    "**how learning actually happens**,\n",
    "not just what the final result is.\n",
    "\n",
    "In the next section,\n",
    "we will implement this process manually\n",
    "by defining the model\n",
    "and writing the training loop.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
